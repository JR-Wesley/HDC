{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "下载好 language 数据集。\n",
    "\n",
    "通过 `torch.utils.data.Dataset` 构建数据集类，实现三个必要的方法，`__init__, __len__, __getitem__`。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Note\n",
    "在Python中，`from typing import ...` 是一种导入语句，用于从 `typing` 模块导入特定的类型注解工具。`typing` 模块提供了用于支持Python类型提示（Type Hints）的工具，这些工具可以帮助开发者为函数参数和返回值指定预期的类型，从而提高代码的可读性和可维护性。\n",
    "\n",
    "以下是你提到的每个导入项的简要说明：\n",
    "\n",
    "1. **`Callable`**：\n",
    "   - `Callable` 是一个类型注解，用于注解一个可调用对象，即任何可以像函数一样被调用的对象。例如，函数、方法、类实例等。\n",
    "   - 示例：`def process_data(func: Callable[[int, str], float]) -> None:` 表示 `process_data` 函数接受一个参数 `func`，它是一个接受一个整数和一个字符串并返回一个浮点数的可调用对象。\n",
    "\n",
    "2. **`Optional`**：\n",
    "   - `Optional` 是一个类型注解，用于注解一个可能为 `None` 的值。\n",
    "   - 示例：`def greet(name: Optional[str]) -> None:` 表示 `greet` 函数接受一个参数 `name`，它可以是一个字符串或者 `None`。\n",
    "\n",
    "3. **`Tuple`**：\n",
    "   - `Tuple` 是一个泛型类型注解，用于注解一个元组，其元素类型是固定的。\n",
    "   - 示例：`def get_coordinates() -> Tuple[int, int]:` 表示 `get_coordinates` 函数返回一个包含两个整数的元组。\n",
    "\n",
    "4. **`List`**：\n",
    "   - `List` 是一个泛型类型注解，用于注解一个列表，其元素类型是固定的。\n",
    "   - 示例：`def get_numbers() -> List[int]:` 表示 `get_numbers` 函数返回一个包含整数的列表。\n",
    "\n",
    "这些类型注解工具使得代码更加清晰，因为它们明确了函数参数和返回值的预期类型。此外，它们还可以被静态类型检查器（如 `mypy`）用来检查类型错误，从而在代码运行之前发现潜在的问题。\n",
    "\n",
    "以下是如何使用这些类型注解的一个示例：\n",
    "\n",
    "```python\n",
    "from typing import Callable, Optional, Tuple, List\n",
    "\n",
    "# 定义一个函数，接受一个整数和一个字符串，返回一个浮点数\n",
    "def process_data(func: Callable[[int, str], float]) -> None:\n",
    "    result = func(10, \"hello\")\n",
    "    print(result)\n",
    "\n",
    "# 定义一个函数，接受一个可能为None的字符串参数\n",
    "def greet(name: Optional[str]) -> None:\n",
    "    if name is not None:\n",
    "        print(f\"Hello, {name}!\")\n",
    "    else:\n",
    "        print(\"Hello, stranger!\")\n",
    "\n",
    "# 定义一个函数，返回一个整数元组\n",
    "def get_coordinates() -> Tuple[int, int]:\n",
    "    return (1, 2)\n",
    "\n",
    "# 定义一个函数，返回一个整数列表\n",
    "def get_numbers() -> List[int]:\n",
    "    return [1, 2, 3]\n",
    "```\n",
    "\n",
    "在这个示例中，我们使用了 `Callable`、`Optional`、`Tuple` 和 `List` 来为函数参数和返回值添加类型注解。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from torch.utils import data\n",
    "from typing import Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuropeanLanguages(data.Dataset):\n",
    "    \"\"\"European Languages dataset.\n",
    "\n",
    "    As used in the paper `\"A Robust and Energy-Efficient Classifier Using\n",
    "    Brain-Inspired Hyperdimensional Computing\" <https://iis-people.ee.ethz.ch/~arahimi/papers/ISLPED16.pdf>`_.\n",
    "    The dataset contains sentences in 21 European languages,\n",
    "    the training data was taken from `Wortschatz Corpora <https://wortschatz.uni-leipzig.de/en/download>`_\n",
    "    and the testing data from `Europarl Parallel Corpus <https://www.statmt.org/europarl/>`_.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where the training and testing samples are located.\n",
    "        train (bool, optional): If True, creates dataset from Wortschatz Corpora,\n",
    "            otherwise from Europarl Parallel Corpus.\n",
    "        download (bool, optional): If True, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in an torch.LongTensor\n",
    "            and returns a transformed version.\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    classes: List[str] = [\n",
    "        \"Bulgarian\",\n",
    "        \"Czech\",\n",
    "        \"Danish\",\n",
    "        \"Dutch\",\n",
    "        \"German\",\n",
    "        \"English\",\n",
    "        \"Estonian\",\n",
    "        \"Finnish\",\n",
    "        \"French\",\n",
    "        \"Greek\",\n",
    "        \"Hungarian\",\n",
    "        \"Italian\",\n",
    "        \"Latvian\",\n",
    "        \"Lithuanian\",\n",
    "        \"Polish\",\n",
    "        \"Portuguese\",\n",
    "        \"Romanian\",\n",
    "        \"Slovak\",\n",
    "        \"Slovenian\",\n",
    "        \"Spanish\",\n",
    "        \"Swedish\",\n",
    "    ]\n",
    "\n",
    "    files: List[str] = [\n",
    "        \"bul.txt\",\n",
    "        \"ces.txt\",\n",
    "        \"dan.txt\",\n",
    "        \"nld.txt\",\n",
    "        \"deu.txt\",\n",
    "        \"eng.txt\",\n",
    "        \"est.txt\",\n",
    "        \"fin.txt\",\n",
    "        \"fra.txt\",\n",
    "        \"ell.txt\",\n",
    "        \"hun.txt\",\n",
    "        \"ita.txt\",\n",
    "        \"lav.txt\",\n",
    "        \"lit.txt\",\n",
    "        \"pol.txt\",\n",
    "        \"por.txt\",\n",
    "        \"ron.txt\",\n",
    "        \"slk.txt\",\n",
    "        \"slv.txt\",\n",
    "        \"spa.txt\",\n",
    "        \"swe.txt\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        root = os.path.join(root, \"Eurolang\")\n",
    "        root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted.\"\n",
    "            )\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.targets.size(0)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, torch.LongTensor]: (sample, target) where target is the index of the target class\n",
    "        \"\"\"\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "        \n",
    "    def _check_integrity(self) -> bool:\n",
    "        \"\"\"\n",
    "        Function\n",
    "        ===\n",
    "        Unzip the dataset file.\n",
    "        Check if `root`  is a legal directory and if the root directory contains the required file\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root):\n",
    "            return False\n",
    "        \n",
    "        train_dir = os.path.join(self.root, \"training\")\n",
    "        has_train_dir = os.path.isdir(train_dir)\n",
    "        test_dir = os.path.join(self.root, \"testing\")\n",
    "        has_test_dir = os.path.isdir(test_dir)\n",
    "        if not has_train_dir or not has_test_dir:\n",
    "            return False\n",
    "\n",
    "        for file in self.files:\n",
    "            has_train_file = os.path.isfile(os.path.join(train_dir, file))\n",
    "            if not has_train_file:\n",
    "                return False\n",
    "            \n",
    "            has_test_file = os.path.isfile(os.path.join(train_dir, file))\n",
    "            if not has_test_file:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_dir = os.path.join(self.root, \"training\" if self.train else \"testing\")\n",
    "\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for class_label, filename in enumerate(self.files):\n",
    "            with open(os.path.join(data_dir, filename), \"r\") as file:\n",
    "                lines = []\n",
    "                for line in file:\n",
    "                    cleaned_line = self._clean_line(line)\n",
    "                    if self._filter_line(cleaned_line):\n",
    "                        lines.append(cleaned_line)\n",
    "\n",
    "                # lines = file.readlines()\n",
    "                # lines = map(self._clean_line, lines)\n",
    "                # lines = filter(self._filter_line, lines)\n",
    "                # lines = list(lines)\n",
    "\n",
    "                data += lines\n",
    "                target += [class_label] * len(lines)\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def _clean_line(self, line):\n",
    "        line = line.strip()  # remove space at start and end\n",
    "        line = \" \".join(line.split())  # compact any whitespace to a single space\n",
    "        return line\n",
    "\n",
    "    def _filter_line(self, line):\n",
    "        return line != \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_SIZE = 128\n",
    "PADDING_IDX = 0\n",
    "\n",
    "ASCII_A = ord(\"a\")\n",
    "ASCII_Z = ord(\"z\")\n",
    "ASCII_SPACE = ord(\" \")\n",
    "NUM_TOKENS = ASCII_Z - ASCII_A + 3  # a through z plus space and padding\n",
    "\n",
    "def char2int(char: str) -> int:\n",
    "    \"\"\"\n",
    "    Func:\n",
    "    Map a character to its integer identifier\n",
    "    \"\"\"\n",
    "    ascii_index = ord(char)\n",
    "\n",
    "    if ascii_index == ASCII_SPACE:\n",
    "        # Remap the space character to come after \"z\"\n",
    "        return ASCII_Z - ASCII_A + 1\n",
    "\n",
    "    return ascii_index - ASCII_A\n",
    "\n",
    "\n",
    "def transform(x: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Func\n",
    "    ===\n",
    "    Transform a string into a tensor of character indeces.\n",
    "    \"\"\"\n",
    "    char_ids = x[:MAX_INPUT_SIZE]\n",
    "    char_ids = [char2int(char) + 1 for char in char_ids.lower()]\n",
    "\n",
    "    if len(char_ids) < MAX_INPUT_SIZE:\n",
    "        char_ids += [PADDING_IDX] * (MAX_INPUT_SIZE - len(char_ids))\n",
    "\n",
    "    return torch.tensor(char_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "train_ds = EuropeanLanguages(\"../data\", train=True, transform=transform, download=True)\n",
    "train_ld = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_ds = EuropeanLanguages(\"../data\", train=False, transform=transform, download=True)\n",
    "test_ld = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
