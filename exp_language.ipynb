{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/maria/py/dl/my_d2l')\n",
    "from my_package import preData\n",
    "from my_package import fig\n",
    "from my_package import hdc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# European Language 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from torch.utils import data\n",
    "from typing import Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuropeanLanguages(data.Dataset):\n",
    "    \"\"\"European Languages dataset.\n",
    "\n",
    "    As used in the paper `\"A Robust and Energy-Efficient Classifier Using\n",
    "    Brain-Inspired Hyperdimensional Computing\" <https://iis-people.ee.ethz.ch/~arahimi/papers/ISLPED16.pdf>`_.\n",
    "    The dataset contains sentences in 21 European languages,\n",
    "    the training data was taken from `Wortschatz Corpora <https://wortschatz.uni-leipzig.de/en/download>`_\n",
    "    and the testing data from `Europarl Parallel Corpus <https://www.statmt.org/europarl/>`_.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where the training and testing samples are located.\n",
    "        train (bool, optional): If True, creates dataset from Wortschatz Corpora,\n",
    "            otherwise from Europarl Parallel Corpus.\n",
    "        download (bool, optional): If True, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in an torch.LongTensor\n",
    "            and returns a transformed version.\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    classes: List[str] = [\n",
    "        \"Bulgarian\",\n",
    "        \"Czech\",\n",
    "        \"Danish\",\n",
    "        \"Dutch\",\n",
    "        \"German\",\n",
    "        \"English\",\n",
    "        \"Estonian\",\n",
    "        \"Finnish\",\n",
    "        \"French\",\n",
    "        \"Greek\",\n",
    "        \"Hungarian\",\n",
    "        \"Italian\",\n",
    "        \"Latvian\",\n",
    "        \"Lithuanian\",\n",
    "        \"Polish\",\n",
    "        \"Portuguese\",\n",
    "        \"Romanian\",\n",
    "        \"Slovak\",\n",
    "        \"Slovenian\",\n",
    "        \"Spanish\",\n",
    "        \"Swedish\",\n",
    "    ]\n",
    "\n",
    "    files: List[str] = [\n",
    "        \"bul.txt\",\n",
    "        \"ces.txt\",\n",
    "        \"dan.txt\",\n",
    "        \"nld.txt\",\n",
    "        \"deu.txt\",\n",
    "        \"eng.txt\",\n",
    "        \"est.txt\",\n",
    "        \"fin.txt\",\n",
    "        \"fra.txt\",\n",
    "        \"ell.txt\",\n",
    "        \"hun.txt\",\n",
    "        \"ita.txt\",\n",
    "        \"lav.txt\",\n",
    "        \"lit.txt\",\n",
    "        \"pol.txt\",\n",
    "        \"por.txt\",\n",
    "        \"ron.txt\",\n",
    "        \"slk.txt\",\n",
    "        \"slv.txt\",\n",
    "        \"spa.txt\",\n",
    "        \"swe.txt\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        root = os.path.join(root, \"Eurolang\")\n",
    "        root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted.\"\n",
    "            )\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.targets.size(0)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, torch.LongTensor]: (sample, target) where target is the index of the target class\n",
    "        \"\"\"\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "        \n",
    "    def _check_integrity(self) -> bool:\n",
    "        \"\"\"\n",
    "        Function\n",
    "        ===\n",
    "        Unzip the dataset file.\n",
    "        Check if `root`  is a legal directory and if the root directory contains the required file\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.root):\n",
    "            return False\n",
    "        \n",
    "        train_dir = os.path.join(self.root, \"training\")\n",
    "        has_train_dir = os.path.isdir(train_dir)\n",
    "        test_dir = os.path.join(self.root, \"testing\")\n",
    "        has_test_dir = os.path.isdir(test_dir)\n",
    "        if not has_train_dir or not has_test_dir:\n",
    "            return False\n",
    "\n",
    "        for file in self.files:\n",
    "            has_train_file = os.path.isfile(os.path.join(train_dir, file))\n",
    "            if not has_train_file:\n",
    "                return False\n",
    "            \n",
    "            has_test_file = os.path.isfile(os.path.join(train_dir, file))\n",
    "            if not has_test_file:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_dir = os.path.join(self.root, \"training\" if self.train else \"testing\")\n",
    "\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for class_label, filename in enumerate(self.files):\n",
    "            with open(os.path.join(data_dir, filename), \"r\") as file:\n",
    "                lines = []\n",
    "                for line in file:\n",
    "                    cleaned_line = self._clean_line(line)\n",
    "                    if self._filter_line(cleaned_line):\n",
    "                        lines.append(cleaned_line)\n",
    "\n",
    "                # lines = file.readlines()\n",
    "                # lines = map(self._clean_line, lines)\n",
    "                # lines = filter(self._filter_line, lines)\n",
    "                # lines = list(lines)\n",
    "\n",
    "                data += lines\n",
    "                target += [class_label] * len(lines)\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def _clean_line(self, line):\n",
    "        line = line.strip()  # remove space at start and end\n",
    "        line = \" \".join(line.split())  # compact any whitespace to a single space\n",
    "        return line\n",
    "\n",
    "    def _filter_line(self, line):\n",
    "        return line != \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_SIZE = 128\n",
    "PADDING_IDX = 0\n",
    "\n",
    "ASCII_A = ord(\"a\")\n",
    "ASCII_Z = ord(\"z\")\n",
    "ASCII_SPACE = ord(\" \")\n",
    "NUM_TOKENS = ASCII_Z - ASCII_A + 3  # a through z plus space and padding\n",
    "\n",
    "def char2int(char: str) -> int:\n",
    "    \"\"\"\n",
    "    Func:\n",
    "    Map a character to its integer identifier\n",
    "    \"\"\"\n",
    "    ascii_index = ord(char)\n",
    "\n",
    "    if ascii_index == ASCII_SPACE:\n",
    "        # Remap the space character to come after \"z\"\n",
    "        return ASCII_Z - ASCII_A + 1\n",
    "\n",
    "    return ascii_index - ASCII_A\n",
    "\n",
    "\n",
    "def transform(x: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Func\n",
    "    ===\n",
    "    Transform a string into a tensor of character indeces.\n",
    "    \"\"\"\n",
    "    char_ids = x[:MAX_INPUT_SIZE]\n",
    "    char_ids = [char2int(char) + 1 for char in char_ids.lower()]\n",
    "\n",
    "    if len(char_ids) < MAX_INPUT_SIZE:\n",
    "        char_ids += [PADDING_IDX] * (MAX_INPUT_SIZE - len(char_ids))\n",
    "\n",
    "    return torch.tensor(char_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "train_ds = EuropeanLanguages(\"../data\", train=True, transform=transform, download=True)\n",
    "train_ld = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_ds = EuropeanLanguages(\"../data\", train=False, transform=transform, download=True)\n",
    "test_ld = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
