{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMG_hand_gestures 中是利用 torchHD 的 model 对 EMG 手势识别进行判别的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Callable, Optional, Tuple, List\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGHandGestures(data.Dataset):\n",
    "    \"\"\"EMG-based hand gestures dataset.\n",
    "\n",
    "    Dataset from the paper `\"Hyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition\" <https://iis-people.ee.ethz.ch/~arahimi/papers/ICRC16.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where files are stored.\n",
    "        download (bool, optional): If True, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in an torch.FloatTensor\n",
    "            and returns a transformed version.\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        subjects (list[int], optional): The subject numbers from 0 til 4 to include. Defaults to [0, 1, 2, 3, 4].\n",
    "        window (int, optional): The number of measurements to include in each sample. Defaults to 256.\n",
    "    \"\"\"\n",
    "\n",
    "    classes: List[str] = [\n",
    "        \"Closed hand\",\n",
    "        \"Open hand\",\n",
    "        \"Two-finger pinch\",\n",
    "        \"Point index\",\n",
    "        \"Rest position\",\n",
    "    ]\n",
    "\n",
    "    features_files = [\n",
    "        \"COMPLETE_1.csv\",\n",
    "        \"COMPLETE_2.csv\",\n",
    "        \"COMPLETE_3.csv\",\n",
    "        \"COMPLETE_4.csv\",\n",
    "        \"COMPLETE_5.csv\",\n",
    "    ]\n",
    "\n",
    "    labels_files = [\n",
    "        \"LABEL_1.csv\",\n",
    "        \"LABEL_2.csv\",\n",
    "        \"LABEL_3.csv\",\n",
    "        \"LABEL_4.csv\",\n",
    "        \"LABEL_5.csv\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        subjects: list = [0, 1, 2, 3, 4],\n",
    "        window: int = 256,\n",
    "    ):\n",
    "        root = os.path.join(root, \"EMG_based_hand_gesture\")\n",
    "        root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.subjects = subjects\n",
    "        self.window = window\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted. You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.FloatTensor, torch.LongTensor]: (sample, target) where target is the index of the target class\n",
    "        \"\"\"\n",
    "        sample = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "    def _check_integrity(self) -> bool:\n",
    "        if not os.path.isdir(self.root):\n",
    "            return False\n",
    "\n",
    "        has_not_feature_files = sum(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: not os.path.isfile(os.path.join(self.root, x)),\n",
    "                    self.features_files,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        has_not_label_files = sum(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: not os.path.isfile(os.path.join(self.root, x)),\n",
    "                    self.labels_files,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # Check if the root directory contains the required files\n",
    "        if not has_not_feature_files and not has_not_label_files:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _load_data(self):\n",
    "        features = torch.empty(0, dtype=torch.long)\n",
    "        labels = torch.empty(0, dtype=torch.long)\n",
    "        for i in self.subjects:\n",
    "            complete = pd.read_csv(\n",
    "                os.path.join(self.root, self.features_files[i]), header=None\n",
    "            )\n",
    "            label = pd.read_csv(\n",
    "                os.path.join(self.root, self.labels_files[i]), header=None\n",
    "            )\n",
    "            # List of indices where the gesture changes\n",
    "            indexes = [\n",
    "                index\n",
    "                for index, _ in enumerate(label.values)\n",
    "                if label.values[index] != label.values[index - 1]\n",
    "            ]\n",
    "            prev = 0\n",
    "            labels_clean = torch.empty(0, dtype=torch.long)\n",
    "            features_clean = torch.empty(0, self.window, 4, dtype=torch.long)\n",
    "            # Every change of gesture we group it values\n",
    "            for j in indexes:\n",
    "                span = j - prev\n",
    "                # If we have that the amount of data of the gesture fits in the window we have a new sample of the gesture of size window\n",
    "                if span > self.window:\n",
    "                    for k in range(math.floor(span / self.window)):\n",
    "                        # Clean the label data\n",
    "                        label_clean = (\n",
    "                            torch.tensor(\n",
    "                                label.values[prev + (self.window * k)], dtype=torch.long\n",
    "                            )\n",
    "                            - 1\n",
    "                        )\n",
    "                        # Clean the feature data\n",
    "                        feature_clean = torch.tensor(\n",
    "                            complete.values[\n",
    "                                prev\n",
    "                                + (self.window * k) : prev\n",
    "                                + (self.window * (k + 1))\n",
    "                            ],\n",
    "                            dtype=torch.long,\n",
    "                        )[None, :, :]\n",
    "                        labels_clean = torch.cat((labels_clean, label_clean))\n",
    "                        features_clean = torch.cat((features_clean, feature_clean))\n",
    "                prev = j\n",
    "            features = torch.cat((features, features_clean))\n",
    "            labels = torch.cat((labels, labels_clean))\n",
    "        self.data = features\n",
    "        self.targets = labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
